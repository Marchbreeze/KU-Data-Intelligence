## 1. 강화학습 개요

### (1) 기계학습 비교

- 기계학습의 종류
    - 지도학습 : 데이터 X와 Y가 주어졌을 때, 둘 사이의 관계를 유추하는 문제
    - 비지도학습 : 데이터 X가 주어졌을 때, 데이터의 형태와 구조를 파악하는 문제
    - 강화학습 (RL, Reinforcement Learning)
        - 주어진 상황에서 최적의 결과를 얻기위한 행동을 결정하는 문제
        - 지도/비지도학습과는 문제의 설정이나 접근법이 상당히 다름
        - 인간이 시도와 실패(trial & error)를 반복하며 문제를 해결하는 것을 모방

- 지도학습과의 차이점
    1. 오라클의 부재
        - 지도학습은 올바른 정답(오라클)이 존재하지만 강화학습은 그렇지 않음
        - 어떤 상황에서의 최적의 행동에 대한 답이 없고 알기도 어려움
    2. 피드백의 희소성
        - 보상함수가 희소한 경우가 많음
        - ex. 바둑의 경우 중간과정에서는 보상을 알 수 없고 종료되고 나서야 승패에 따라 보상이 주어짐
    3. 훈련과 데이터의 동시성
        - 훈련 과정 중에 학습 데이터가 생성되기 때문에 임의로 추출된 데이터가 아님
        - 모델에 따라 수집되는 데이터가 달라지고, 거기에 따라 모델이 달라짐

### (2) 강화학습(RL)

- 강화학습 문제의 표현
    - 에이전트(agent)와 환경(environment)으로 구성되는 시스템
    - 상태 → 행동 → 보상의 주기를 반복하며 보상을 최대로 하고자 함
        - 환경 : 어떠한 상태(state)에 위치
        - 에이전트 : 어떠한 행동(action)을 하여 환경의 상태를 변화
        - 변화된 환경 : 그에 따른 보상(reward) 신호를 에이전트에 제공
            
            ![2024-12-20_02-52-41.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/882ae667-93bc-4b62-aec3-0925eb9258e7/2024-12-20_02-52-41.jpg)
            

- 마르코프 결정 과정 (MDP, Markov Decision Process)
    - 가정 : 다음 상태는 현재 상태와 행동에 의해서만 결정, 그 이전의 상태/행동과는 무관
    - MDP의 구성 : 상태집합, 행동집합, 전이함수, 보상함수
    1. 전이함수
        - 상태 $𝑠_𝑡$에서 행동 $𝑎_𝑡$를 했을 때, 다음 상태 $𝑠_{𝑡+1}$로 전이되는 것을 표현하는 함수 (확률)
            
            ![2024-12-20_02-56-29.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/233adb7e-99c3-44ac-8c4d-6adc97c7983d/2024-12-20_02-56-29.jpg)
            
    2. 보상함수
        - 상태 $𝑠_𝑡$에서 행동 $𝑎_𝑡$를 통해 상태 $𝑠_{𝑡+1}$로 전이되었을 때 얻어지는 보상
            
            ![2024-12-20_02-57-13.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/d72eddc8-779d-45db-9145-20a0d8d9f50d/2024-12-20_02-57-13.jpg)
            
    - 전이함수와 보상함수를 모르는 상태에서 경험적으로 얻어지는 ($𝑠_𝑡$, $𝑎_𝑡$, $𝑟_𝑡$)를 통해서 최선의 행동을 결정

- 보상의 수학적 표현
    1. 목적함수(J)
        - 어떤 정책(policy) 𝝅에 따라 행동을 결정했을 때 얻어지는 평균적인 이득
            
            ![2024-12-20_03-06-10.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/7d43f923-3afa-4665-b4c8-ccb94b51027c/2024-12-20_03-06-10.jpg)
            
    2. 이득(R)
        - 어떤 경로 𝜏 = ($𝑠_0$, $𝑎_0$, $𝑟_0$) ,($𝑠_1$, $𝑎_1$, $𝑟_1$)…($𝑠_𝑡$, $𝑎_𝑡$, $𝑟_𝑡$)를 통해 에피소드가 진행되었을 때,
        이 에이전트가 받는 할인된 보상의 합 (할인률 적용됨)
            
            ![2024-12-20_03-04-07.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/ccdc37f9-3c88-4e4c-bd24-59d4441dcf39/2024-12-20_03-04-07.jpg)
            
    3. 할인률(𝛾)
        - 미래 보상에 대한 현 시점에서의 할인 (0 ≤ 𝛾 ≤ 1)
        - **할인된 보상 :** 미래의 보상이 현재 보상보다 덜 중요하게 여겨지는 개념 - 할인율에 의해 조정
        - 다음 상태나 보상이 확률적으로 결정되기 때문에 이득 또한 확률적으로 주어짐

- 학습되는 주요 함수
    1. 정책함수(policy function)
        - 현재 상태에서 행동을 결정하는 함수, $𝑎 = 𝜋(𝑠)$
    2. 가치함수(value function)
        - 현재 상태에서 예상되는 이득을 추정하는 함수 (예상 이득: 앞으로 발생할 모든 보상의 합)
        - 모든 행동에 대한 평균적인 예상 이득: $𝑉^𝜋(𝑠) = 𝐸_{𝜏𝜋}[ 𝑅 (𝜏)]$
        - 특정 행동의 했을 때의 예상 이득: $𝑄^𝜋(𝑠, 𝑎) = 𝐸_{𝜏𝜋} [𝑅 (𝜏)]$
    3. 전이함수(transition function)
        - 현 상태에서 다음 상태로 전이를 결정, $𝑃 (𝑠^′| 𝑠, 𝑎)$

- 학습 과정
    1. 정책(초기에는 임의)에 따라 에피소드 ($𝑠_0$, $𝑎_0$, $𝑟_0$) ,($𝑠_1$, $𝑎_1$, $𝑟_1$)…($𝑠_𝑡$, $𝑎_𝑡$, $𝑟_𝑡$) 생성
    2. 정책/가치/전이함수 중 하나 (혹은 두 개)를 추정
    3. 추정된 함수에 따라 최선의 정책을 도출하여 업데이트
    - 목적을 달성할 때까지 1~3 반복

- 전통적 강화학습: 각 정책/가치/전이함수를 테이블 혹은 간단한 함수로 표현
    - 장점: 학습이 간단하고 환경에 대하여 이해가 빠름
    - 단점: 복잡한 시스템에 대한 묘사가 어려움
    
- 심층 강화학습 (DRL, Deep Reinforcement Learning)
    - 각 함수를 심층 인공신경망을 이용해 표현
    - 장점
        - 복잡한 시스템에 대하여 비선형적인 표현이 가능
        - 비정형 데이터로 표현되는 상태와 행동에 대한 고려가 가능
    - 단점
        - 학습이 어려움
    - 현재의 대부분의 강화학습에 사용되고 있고, 이전에는 불가능했던 많은 문제를 인간 수준 이상으로 푸는 것이 가능함 (ex. 알파고)

## 2. 주요 강화학습 알고리즘

- 알고리즘 종류
    1. 정책기반 알고리즘
        - 목적함수를 최대로 하는 정책함수를 추정
        - 직관적이고 모든 종류의 행동에 적용가능하지만 데이터가 많이 필요
        - ex. REINFORCE
    2. 가치기반 알고리즘
        - 가치함수를 학습하고 그로부터 정책을 결정
        - 데이터가 상대적으로 적게 필요하지만 최적 정책에 대한 보장이 어려움
        - Q-learning : $𝑄^𝜋(𝑠, 𝑎)$를 주로 학습 (특정 행동을 했을 떄의 예상 이득)
        - ex. SARSA, DQN
    3. 모델기반 알고리즘
        - 전이함수를 학습하거나 이미 알려진 모델을 사용
        - 모델이 정확하면 제일 좋지만 대부분 좋은 모델을 찾기 어려움
        - ex. 몬테카를로 트리 탐색 (MCTS)
    4. 결합
        - 두 개 이상의 접근법이 결합되어 적용
        - ex. 가치+정책 : A2C, PPO
        - ex. 가치+정책+모델 : AlphaGo

### (1) REINFORCE (정책 기반)

- REINFORCE
    - 정책경사(policy gradient)를 이용하여 최적화

- 수학적 표현
    1. 정책함수 $𝜋_𝜃(𝑠)$
        - 𝜃를 파라미터로 하고 입력이 상태, 출력이 행동인 함수
        - 인공신경망으로 표현한다면 𝜃는 신경망의 가중치
    2. 목적함수 $𝐽(𝜋_𝜃)$
        - $𝜋_𝜃$를 따라서 행동할 때의 평균 이득(return) : $𝐸[ 𝑅 (𝜏)]$
    3. 경사하강법
        - 경사하강법을 이용하여 목적함수를 최대화하는 𝜃를 찾음
        - $𝐽(𝜋_𝜃)$를 𝜃로 미분한 경사도를 구하고 이를 이용해 𝜃를 업데이트
            
            ![2024-12-20_03-44-23.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/0c8eb43f-6c49-415b-9b2a-d5049b0893ad/2024-12-20_03-44-23.jpg)
            
            - 정책경사 (policy gradient) $∇_𝜃𝐽 (𝜋_𝜃)$
                - 여러 경우(𝜏)에 대한 평균이지만 하나의 표본을 이용해 계산
                - 일반 경사하강법과 확률적 경사하강법의 관계와 유사

- ex. 얼어있는 호수에서 구멍을 피해 목적지에 도달하는 문제
    - 얼어있는 호수가 가로로 길게 있고, 행동은 좌우로 한 칸씩 이동하는 것을 가정
    - 𝑠0 에서 시작하여 구멍이나 목적지(땅)에 도달하면 에피소드가 종료
    - 목적지에 도달하면 보상 획득
    - 정책함수 : $𝜋_𝜃(𝑠) = 𝜃$, 오른쪽으로 이동할 확률
        
        ![2024-12-20_03-45-31.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/e4076d20-0212-4ed5-9066-e17a10f964e0/2024-12-20_03-45-31.jpg)
        
        - 정책함수 초기화 후 보상 계산
            
            ![2024-12-20_03-47-36.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/0fddcff1-fc26-4cce-acb2-d6b6c76385b0/2024-12-20_03-47-36.jpg)
            
        - 할인율=1, 학습률=0.05로 설정
        - 정책경사는 각 행동 R에 대해서는 $𝜃^{−1}$, L에 대해서는 $− (1 − 𝜃)^{−1}$로 주어짐
        - 정책경사에 이득을 곱해서 계산 (자세한 과정은 생략)
            
            ![2024-12-20_03-49-16.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/0e3fb6e0-3663-4602-9c86-5167a81f605c/2024-12-20_03-49-16.jpg)
            
    - 최종적으로 𝜃는 1에 수렴
    - 할인율의 효과
        - 할인율이 1이기 때문에 빨리 도달하는 것에 대한 이득이 없어 수렴이 느림
        - 할인율을 1보다 작게 설정하면 빨리 도달하는 것에 대한 이득이 높아 수렴이 빠름
            - L로 인한 정책경사 감소의 효과가 줄어듦

### (2) DQN (가치 기반)

- Q-learning
    - $𝑄^𝜋(𝑠, 𝑎)$ : 상태 𝑠에서 행동 𝑎를 하였을 때 예상되는 평균 이득
    - $V^𝜋(𝑠)$ : 상태 𝑠에서 예상되는 평균 이득, Q함수를 행동에 대해 평균
        - 최적의 행동을 결정하기 쉽기 때문에 보통은 Q 함수를 추정

- Q 함수에서의 표현
    - 현재 t=0 상태의 Q함수 : $𝑄^𝜋(𝑠, 𝑎) = E[R_0(𝜏)] = E[r_0+𝛾r_1+𝛾^2r_2+...+𝛾^Tr_T]$
    - 다음 t=1 상태의 Q함수 : $𝑄^𝜋(𝑠, 𝑎) = E[R_1(𝜏)] = E[r_1+𝛾r_2+𝛾^2r_3+...+𝛾^{T-1}r_T]$
    
- 벨만 방정식
    - 현재 Q함수는 다음 시점의 Q함수로 재귀적으로 표현
    - (𝑠, 𝑎, 𝑟): 현재의 상태, 행동, 보상 & (𝑠’, 𝑎’, 𝑟’): 다음 시점의 상태, 행동, 보상
    - $𝑄^𝜋(𝑠, 𝑎) = r + 𝛾*E[𝑄^𝜋(𝑠^’, 𝑎^’)]$
        - $E[𝑄^𝜋(𝑠^’, 𝑎^’)]$ : 다음 시점에서 예상되는 행동에 대한 평균, 𝑠′은 결정됨
        - 이것을 적절히 추정하는 것이 Q학습의 핵심

- DQN (Deep Q-Network)
    - $E[𝑄^𝜋(𝑠^’, 𝑎^’)]$를 최대값을 이용해 추정
        - $𝑄^𝜋(𝑠, 𝑎) = r + 𝛾*max_{a}[𝑄^𝜋(𝑠^’, 𝑎^’)]$
    - 보상을 최대로하는 정책을 찾는 것이 목표이기 때문에, 이는 최적의 Q함수를 학습
    - $max_{a}[𝑄^𝜋(𝑠^’, 𝑎^’)]$ 는 다음 번 행동 𝑎′ 에 따라 변하지 않음
        - 비활성정책 알고리즘: 현재 정책에 따라 학습이 영향받지 않음
        - 경험재현: 이전 에피소드의 재활용이 가능하여 샘플 효율이 좋음
    - 최대값을 구해야하기 때문에 이산적 행동 공간(R or L)에서만 적용 가능

- 가치망(Value network, Q-Network)
    - 하나의 에피소드에 대하여 Q함수의 실제값이 계산되고, 이 값을 y로 하여 Q함수를 학습
    - Q함수를 심층 신경망으로 모델링
        - 비선형적 시스템을 예측 → 비정형으로 주어지는 환경 변수를 처리하는 것이 가능
        - 일반화를 통해 경험하지 않은 환경과 행동에 대한 예측

- 훈련 과정
    1. Q함수를 초기화, 보통은 보상을 모르므로 0으로 초기화
    2. Q함수로부터 행동을 선택하여 하나의 에피소드 생성 (Q함수를 학습시키기에 적절한 에피소드를 생성)
    3. 에피소드로부터 Q함수의 실제값을 계산하고 Q함수를 업데이트
    - 위의 과정을 반복

- 에피소드 생성 방법 (행동선택)
    1. 탐욕정책
        - 보상이 가장 큰 행동을 선택
        - 선택이 제한적이고 아직 탐험하지 못한 행동에 대해 탐험이 어려움
    2. 엡실론 탐욕정책
        - 1 − 𝜖의 확률로 탐욕적 선택, 𝜖의 확률로 임의 선택
        - 다양한 탐색이 가능하지만 임의 탐색에서 좀더 좋은 선택에 대한 고려가 없음
    3. 볼츠만 정책
        - 소프트맥스를 이용해 선택 확률을 결정
        - 𝜏: 확률을 얼마나 균일하게 할지 조절
        - 유사한 Q에 대해 비슷하게 선택
    - 탐험(exploration)과 탐욕(활용, exploitation)의 균형
        - 좋은 행동을 찾기위해 안 해본 선택을 시도하는 것이 중요
        - 동시에 이미 학습한 좋은 행동을 활용하는 것도 중요
        
- ex. Frozen Lake
    1. 반복1
        - 업데이트 전 Q함수 (0으로 초기화)
            
            ![2024-12-20_04-29-48.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/418d026a-edd3-4cfb-b4f7-5c0ca172e5f3/2024-12-20_04-29-48.jpg)
            
        - 에피소드 $r_1$ : RLRRR
            
            ![2024-12-20_04-30-29.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/65f1f51d-2369-4d38-9d15-d55d24e7d49c/2024-12-20_04-30-29.jpg)
            
        - 업데이트 후 Q함수
            
            ![2024-12-20_04-30-58.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/8bf374d3-d286-4b49-8049-0b2eb7077335/2024-12-20_04-30-58.jpg)
            
    2. 반복2
        - 에피소드 $r_2$ : RRR
            
            ![2024-12-20_04-39-46.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/fd4811bc-8c7c-4f2f-be8f-c18aea4afba6/2024-12-20_04-39-46.jpg)
            
        - 업데이트 후 Q함수
            
            ![2024-12-20_04-40-06.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/fc14b679-04eb-4059-805d-64ee7b5816d2/2024-12-20_04-40-06.jpg)
            
    3. 반복3
        - 에피소드 $r_3$ : RRLRR
            
            ![2024-12-20_04-40-45.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/ecd70b79-e57f-4c7d-8437-effb2796aafe/2024-12-20_04-40-45.jpg)
            
        - 업데이트 후 Q함수
            
            ![2024-12-20_04-40-58.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/6b6f3e95-0c3f-40c8-a9e1-7784bcf8a602/2024-12-20_04-40-58.jpg)
            

- 구현
    1. Fronzen Lake 예제 설정
        
        ```python
        # 상태: 구멍(H)-출발(S)-얼음(F)-얼음(F)-목적지(G) 의 구조
        # 행동: 0(LEFT), 2(RIGHT)
        # 보상: G은 +1, 나머지는 0
        env = gym.make('FrozenLake-v1',desc=['HSFFG'],is_slippery=False)
        ```
        
    2. 가치함수 & 정책함수
        
        ```python
        # 가치함수: Q테이블, 0으로 초기화
        Q = np.zeros([env.observation_space.n, env.action_space.n])
        ```
        
        ![2024-12-20_04-54-43.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/f1736948-167b-4237-9906-7c7eea96675e/2024-12-20_04-54-43.jpg)
        
        ```python
        # 정책 함수, e는 e-greedy 변수
        # 행동에 대한 Q값(L,R)을 받아 행동을 결정
        def policy(qs,e=0):
          if e > random.random():
            return random.choice([0,2])
          else:
            if qs[0] > qs[2]: return 0
            elif qs[0] < qs[2]: return 2
            else: return random.choice([0,2])
        ```
        
    3. 모델 훈련
        
        ```python
        num_episodes = 100   # 필요하다면 더 늘려서 수행
        discount = 0.9
        Q = Q*0.0  # Q함수 초기화
        for i in range(num_episodes):
          state = env.reset()   # 초기 상태로 리셋
          done = False
          while not done:
            action = policy(Q[state, :],e=0.1) # 행동 결정
            new, reward, done, _ = env.step(action) # 행동에 따른 보상과 새로운 상태
            Q[state,action] = reward + discount*np.max(Q[new, :]) # Q함수 업데이트
            state = new  # 상태 업데이트
        
        Q[1:-1,[0,2]]  # 최종 Q함수
        ```
        
        ![2024-12-20_04-55-46.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/b2b3b850-1673-4181-8780-c6cf3854eb13/2024-12-20_04-55-46.jpg)
        

### (3) A2C (정책 + 가치)

- 보상을 직접 이용하여 정책을 학습하는 것은 불안정
    - ex. 바둑처럼 끝날 때까지 보상이 없는 경우 학습이 어려움
    - 보상대신에 적절한 강화신호를 생성하여 학습하고자 함

- Actor-Critic (AC)
    - 보상 대신에 적절한 신호를 생성해 행동을 강화하는 방식을 사용
    - 어드밴티지(Advantage)의 활용
        - 가치함수로 V, Q 대신에 상대적 평가인 A를 사용
        - 현재 행동이 평균적인 행동보다 얼마나 좋은지 측정 : $A(s,a) = Q(s,a) - V(s)$
        - 평균적으로 0이기 때문에 학습 안정화 & 오버피팅 방지
        - 좋은 상태에 머무르지 않고 더 좋은 상태로 진화하는 것이 가능
    1. 행동자(Actor)
        - 행동 결정을 위한 정책함수 → 어드밴티지 최대화
        - 비평자의 강화신호를 통해 학습
        - $∇_𝜃𝐽 (𝜋_𝜃) = ∇_𝜃E[A(𝜏)]$
    2. 비평자(Critic)
        - 행동자의 결정을 평가하기 위한 가치함수, 강화신호 생성
        - V를 먼저 추정하고 Q를 계산
            - $𝐴(𝑠, 𝑎) ≈ 𝑟 + 𝛾*𝑉(𝑠^′) − 𝑉(𝑠)$
        - 일반적 어드밴티지 추정 (GAE: Generalized Advantage Estimation)
            - 미래에 대한 어드밴티지의 할인된 총합
            - $𝐴( 𝜏) = 𝐴( 𝑠_0, 𝑎_0) + 𝛾_𝐴*𝐴 (𝑠_1, 𝑎_1) + 𝛾_𝐴^2*𝐴 (𝑠_2, 𝑎_2) + …$

- A2C (Advantage Actor-Critic)
    - 신경망 구조
    - Dueling DQN (DQN의 변형)
        - 기존 DQN : 신경망을 통해 Q값을 바로 예측
        - Dueling DQN : A와 V를 학습하고 합쳐서 Q를 예측
    - 방식
        - AC Type 1: 처음에는 정책망을 공유하다가 중간에 정책 함수와 가치 함수의 파라미터를 공유
        - AC Type 2: V와 정책망을 완전히 독립적으로 운영하는 방식
        
        ![2024-12-20_04-49-19.jpg](https://prod-files-secure.s3.us-west-2.amazonaws.com/edfd69d1-6c01-4d0c-9269-1bae8a4e3915/8a03b6b1-c605-482c-875b-e35c3fb4ee33/2024-12-20_04-49-19.jpg)
